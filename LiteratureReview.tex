\section{Literature Review}
\label{sec:lit}

\subsection{Cloud Streaming and Latency Reduction}
Within the last decade the cloud computing space has expanded rapidly and with it the possibilities. Today, even individuals can set up an experimental cloud (\acrfull{vr}) gaming streaming solution from pre-made components \parencite{tayoexe} \parencite{clouddesktopguide}. For less experimentally inclined customers, there are complete services, such as the one from cloud computing company Shadow \parencite{shadow} who recently announced a closed beta for their dedicated VR streaming service \parencite{shadowvr}. Other major players in the cloud gaming scene are Google's Stadia \parencite{stadia}, Microsoft's XBox XCloud \parencite{xcloud} and Nvidia's GeForceNow \parencite{geforcenow}, all of which were launched recently ($>$1 year old (Stadia, GeForceNow)) or have not even been released to the public (XCloud) . Early releases, especially Stadia, were quickly overwhelmed on launch and faced public scrutiny for failing to living up to their promises of turning any device into a gaming computer \parencite{stadiaFail}.  Since then those services made improvements to their \acrfull{qoe} and transitioned into a mainstream technology service.

To facilitate the needed \acrshort{qoe}, cutting edge technology is used to enable the necessary performance. Modern video compression codecs, like the AV1 codec introduced in 2018 \parencite{av1}, are getting better at compressing high-resolution video streams and together with an application like WebRTC \parencite{webRTC} which offers latency optimizations via peer-to-peer networking and more, they lay the foundation for modern cloud streaming applications. Technologies like Google's Seurat Image\hyp{}Based Scene Simplification System \parencite{seurat} and the Shading atlas streaming technique developed by the Graz University of Technology \parencite{sas} offer even further optimization's in areas other than networking and transmitting data. 

Research Papers like the ones from \cite{cutcord} or from \cite{mvr} demonstrate the viability and technical feasibility of cloud \acrshort{vr} streaming . They developed solutions to achieve and undercut the 20 \acrfull{ms} \acrfull{mtp} barrier while streaming \acrshort{vr} content. 20\acrshort{ms} is the agreed upon threshold between receiving user head movement to displaying the frame on the \acrfull{hmd}, to avoid inducing motion sickness \parencite{valvevrlatency}. One such solution is a low latency control loop that streams \acrshort{vr} scenes containing only the userâ€™s \acrfull{fov} and a latency adaptive margin area around the \acrshort{fov}. The additional margin allows the clients to render locally at a high refresh rate and compensate for the head movements before the next frame arrives, all of which contributes to the \acrshort{qoe} \parencite{mvr}. The technique known as 'Adaptive \acrshort{fov}' was explored by a multitude of research papers. In essence the optimization is to send only what the user sees (their \acrshort{fov}) and an adaptive area around it, to facilitate for local head movement before the next frame arrives. The idea of rendering only what the user has to see to keep up the immersion is well established within the game development community. View Frustum culling and Occlusion culling \parencite{cullingdefinition} are widely used in games to increase performance, whereas Adaptive \acrshort{fov} aims to decrease latency by reducing the payload of network transmissions. Yet another angle of attack leverage's the power of parallel rendering, encoding, transmission and decoding, together with a Remote VSync Driven Rendering approach to minimize \acrshort{mtp} latency \parencite{cutcord}. The prototype for that experiment was based on commodity hardware, which further demonstrates the feasibility of cloud \acrshort{vr} streaming. 

%\subsection{Security} 